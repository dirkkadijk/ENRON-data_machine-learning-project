{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.  SVM    (support vector machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 192.657 s\n",
      "predicing time: 20.081 s\n",
      "The accuracy of the prediction of labels of the test features is: 0.9840728100113766\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "##### your code goes here we handle the import statement and SVC creation for you here\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=1.0, gamma='scale')\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "### fit the classifier on the training features and labels\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "##########\n",
    "\n",
    "t0 = time()\n",
    "#### store your predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "print (\"predicing time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "#########\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(pred, labels_test)\n",
    "print('The accuracy of the prediction of labels of the test features is:', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Go to the svm directory to find the starter code (svm/svm_author_id.py). Import, create, train and make predictions with the sklearn SVC classifier. When creating the classifier, use a linear kernel (if you forget this step, you will be unpleasantly surprised by how long the classifier takes to train). What is the accuracy of the classifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 194.122 s\n",
      "881\n",
      "prediction time: 20.304 s\n",
      "0.9840728100113766\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'linear', C=1)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print (\"prediction time:\", round(time()-t0, 3), \"s\")\n",
    "print(clf.score(features_test, labels_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "One way to speed up an algorithm is to train it on a smaller training dataset. The tradeoff is that the accuracy almost always goes down when you do this. Let’s explore this more concretely: add in the following two lines immediately before training your classifier.\n",
    "\n",
    "features_train = features_train[:len(features_train)/100] labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "These lines effectively slice the training dataset down to 1% of its original size, tossing out 99% of the training data. You can leave all other code unchanged. What’s the accuracy now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.102 s\n",
      "1046\n",
      "prediction time: 0.941 s\n",
      "0.8845278725824801\n"
     ]
    }
   ],
   "source": [
    "features_train = features_train[:int(len(features_train)/100)] \n",
    "labels_train = labels_train[:int(len(labels_train)/100)] \n",
    "\n",
    "clf = SVC(kernel = 'linear', C=1)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print (\"prediction time:\", round(time()-t0, 3), \"s\")\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Keep the training set slice code from the last quiz, so that you are still training on only 1% of the full training set. Change the kernel of your SVM to “rbf”. What’s the accuracy now, with this more complex kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-c2439d8ef435>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-c2439d8ef435>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print \"training time:\", round(time()-t0, 3), \"s\"\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'rbf', C=1)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print (\"prediction time:\", round(time()-t0, 3), \"s\")\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the training set size and rbf kernel from the last quiz, but try several values of C (say, 10.0, 100., 1000., and 10000.). Which one gives the best accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'rbf', C=10)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print (\"prediction time:\", round(time()-t0, 3), \"s\")\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'rbf', C=1000)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print (\"prediction time:\", round(time()-t0, 3), \"s\")\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'rbf', C=10000)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print (\"prediction time:\", round(time()-t0, 3), \"s\")\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve optimized C for the RBF kernel, go back to using the full training set. In general, having a larger training set will improve the performance of your algorithm, so (by tuning C and training on a large dataset) we should get a fairly optimized result. What is the accuracy of the optimized SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.118 s\n",
      "931\n",
      "prediction time: 1.028 s\n",
      "0.8998862343572241\n"
     ]
    }
   ],
   "source": [
    "features_train = features_train[:int(len(features_train)/1)] \n",
    "labels_train = labels_train[:int(len(labels_train)/1)] \n",
    "\n",
    "clf = SVC(kernel = 'rbf', C=10000)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "t0 = time()\n",
    "print(sum(clf.predict(features_test) ==1))\n",
    "print (\"prediction time:\", round(time()-t0, 3), \"s\")\n",
    "print(clf.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What class does your SVM (0 or 1, corresponding to Sara and Chris respectively) predict for element 10 of the test set? The 26th? The 50th? (Use the RBF kernel, C=10000, and 1% of the training set. Normally you'd get the best results using the full training set, but we found that using 1% sped up the computation considerably and did not change our results--so feel free to use that shortcut here.) And just to be clear, the data point numbers that we give here (10, 26, 50) assume a zero-indexed list. So the correct answer for element #100 would be found using something like answer=predictions[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for element 10th, 26th and 50th are: 1 0 1\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(features_test)\n",
    "print(\"Prediction for element 10th, 26th and 50th are:\", pred[10], pred[26], pred[50]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 1700 test events--how many are predicted to be in the “Chris” (1) class? (Use the RBF kernel, C=10000., and the full training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events predicted in Chris class is 931\n"
     ]
    }
   ],
   "source": [
    "print('Number of events predicted in Chris class is', sum(clf.predict(features_test) ==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
